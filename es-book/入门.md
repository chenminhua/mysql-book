### jvm 配置建议 config/jvm.options

- Xmx 和 Xms 设置成一样，
- Xmx 不要超过机器内存的一半，
- 不要超过 30G。

## 下载，启动，测试，安装插件

```sh
# 启动elasticsearch，访问 localhost:9200
bin/elasticsearch

# list es 的插件
bin/elasticsearch-plugin list

# 安装 国际化分词 插件
bin/elasticsearch-plugin install analysis-icu
# localhost:9200/_cat/plugins
```

运行多实例的 es

```sh
bin/elasticsearch -E node.name=node1 -E cluster.name=geektime -E path.data=node1_data -d
bin/elasticsearch -E node.name=node2 -E cluster.name=geektime -E path.data=node2_data -d
bin/elasticsearch -E node.name=node3 -E cluster.name=geektime -E path.data=node3_data -d
# localhost:9200/_cat/nodes  查看当前集群运行着哪些节点。
```

## kibana

```sh
bin/kibana
# localhost:5601

bin/kibana-plugin install <plugin_location>
bin/kibana-plugin list
bin/kibana-plugin remove
```

- dashboard
- devtools (kibana console, search profiler)
- kibana plugins

#### run with docker-compose and cerebro

https://github.com/geektime-geekbang/geektime-ELK/tree/master/part-1/2.3-%E5%9C%A8Docker%E5%AE%B9%E5%99%A8%E4%B8%AD%E8%BF%90%E8%A1%8CElasticsearch%2CKibana%E5%92%8CCerebro

```
docker run -p 9000:9000 lmenezes/cerebro
```

## logstash

https://github.com/geektime-geekbang/geektime-ELK/tree/master/part-1/2.4-Logstash%E5%AE%89%E8%A3%85%E4%B8%8E%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE

```logstash
#bin/logstash -f /YOUR_PATH_of_logstash.conf

input {
  file {
    path => "/Users/chenminhua/Downloads/ml-latest-small/movies.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}
filter {
  csv {
    separator => ","
    columns => ["id","content","genre"]
  }

  mutate {
    split => { "genre" => "|" }
    remove_field => ["path", "host","@timestamp","message"]
  }

  mutate {

    split => ["content", "("]
    add_field => { "title" => "%{[content][0]}"}
    add_field => { "year" => "%{[content][1]}"}
  }

  mutate {
    convert => {
      "year" => "integer"
    }
    strip => ["title"]
    remove_field => ["path", "host","@timestamp","message","content"]
  }

}
output {
   elasticsearch {
     hosts => "http://localhost:9200"
     index => "movies"
     document_id => "%{id}"
   }
  stdout {}
}
```

## 查看我们的 es 集群情况

## 文档元数据

- \_index 文档所属的索引
- \_type 文档所属的类型名
- \_id 文档唯一 Id
- \_source 文档的原始 json 数据
- \_version 文档版本信息
- \_score 相关性打分

#### index

- index 逻辑空间的概念，每个索引都有自己的 mapping 定义，用于定义包含的文档的字段名和类型。
- shard 体现物理空间概念。
- index 中的数据分散在 shard 上。
- mapping 定义文档字段的类型。
- setting 定义不同的数据分布。

从 7.0 开始，每个索引只能创建一个 type: "doc"。

## rest api

kibana 中提供了一个管理功能，里面有索引管理功能。里面可以看到 settings, mapping 等信息。

```
GET movies
GET movies/_count
GET movies/_search

# 查看kibana*的indices
GET /_cat/indices/kibana*?v&s=index

# 查看状态为yello的indices
GET /_cat/indices?v&health=yello

# 按照文档数排序
GET /_cat/indices?v&s=docs.count:desc

# 查看具体的字段
GET /_cat/indices/kibana*?pri&v&h=health,index,pri,rep,docs.count,mt
```

## 文档的 CRUD

```sh
# index: 如果id不存在，创建新的文档。否则删除现有文档再创建新的文档，版本会增加。
PUT my_index/_doc/1
{"user": "mike", "comment": "you know, for search"}

# create: 如果id已经存在，会失败
PUT my_index/_create/1
{"user": "mike", "comment": "you know, for search"}
POST my_index/_doc （不指定id，自动生成）
{"user": "mike", "comment": "you know, for search"}

# read
GET my_index/_doc/1

# update: 文档必须已经存在，更新只会对相应字段进行增量修改
POST my_index/_update/1
{"doc": {"user": "mike", "comment": "You know, elasticsearch"}}

# delete
DELETE my_index/_doc/1
```

BULK API

MGET api

MSEARCH

## 倒排索引

- 单词词典（Term Dictionary），记录所有文档的单词，记录单词到倒排列表的关联关系。
- 倒排列表（Posting List），倒排索引项由一下内容组成
  -- 文档 ID
  -- 词频 TF，单词在文档中出现的次数，用于相关性分析
  -- 位置，单词在文档中分词的位置。
  -- 偏移，记录单词的开始结束位置。

可以指定某些字段不做索引。优点是节省空间，缺点是该字段无法被搜索。

## Analysis 与 Analyzer (分词与分词器)

- analysis 是把全文本转换成一系列单词(term, token)的过程，也叫分词。
- analysis 是通过 analyzer 实现的。

analyzer 由三部分组成

- character filters（针对原始文本处理，例如去掉 html）
- tokenizer（按照规则切分为单词）
- token filter（将切分的单词进行加工，小写，删除 stopwords，增加同义词等等）
- 比如 Mastering Elasticsearch & Elasticsearch in Action 分词后得到的是 master, elasticsearch, action。

内置分词器

- Standard Analyzer，默认分词器，按词切分，小写处理
- Simple Analyzer，按非字母切分，小写处理
- Stop Analyzer，小写，停用词过滤
- Whitespace Analyzer，按空格切分，不转小写
- Keyword Analyzer，不分词，直接将输入当输出
- Patter Analyzer，正则表达式，默认\W+（非字符分隔）
- Language，提供了 30 多种常见语言分词器
- Customer Analyzer，自定义分词器

中文分词器

- icu-analyzer
- ik 支持自定义词库，支持热更新分词字典
- thulac 清华大学推出的一套中文分词器

#### 使用 \_analyzer API

```sh
GET /_analyze
{
  "analyzer": "standard",
  "text": ["hello world"]
}

GET /_analyze
{
  "analyzer": "icu_analyzer",
  "text": ["他说的确实在理"]
}
```

## index template 和 dynamic template

index template 帮助你设置 mapping 和 setting，并按照一定规则应用到新建的索引上。

```sh
PUT _template/template_default
{
    "index_patterns": ["*"],
    "order": 0,
    "version": 1,
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 2
    }
}

PUT /_template/template_test
{
    "index_pattern": ["test*"],
    "order": 1,
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 1
    },
    "mappings": {
        "date_detection": false,
        "numeric_detection": true
    }
}

# 查看 template
GET /_template/template_default
GET /_template/temp*
```

什么是 dynamic template？根据 es 识别的数据类型，结合字段名称来动态设定字段类型。比如

- 所有字符串类型都设置为 Keyword
- is 开头的字段都设为 boolean
- long\_ 开头的都设置为 long 类型

## 聚合分析

实时性很高。通过聚合可以得到一个数据的概览。分析和总结全套的数据，而不是查找单个文档。比如

- 尖沙咀和香港岛的客房数量。
- 不同的价格区间，可预定的经济型酒店和五星级酒店数量。

高性能，只需要一条语句，就可以从 es 得到分析结果。无需在客户端自己去实现逻辑分析。kibana 可视化报表就可以通过 es 的聚合功能实现。

- bucket aggregation 一些列满足特定条件的文档的集合
- metric aggregation 对其他的聚合结果进行二次聚合
- pipeline aggregation 对其他的聚合结果进行二次聚合
- matrix aggregation 支持对多个字段的操作并提供一个结果矩阵
