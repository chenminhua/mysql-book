发布订阅的对象是主题 （Topic）。发布消息（Record）的是生产者（producer），订阅主题消息的是消费者（consumer），生产者和消费者都是客户端（clients）。

kafka 服务端由 broker 服务进程组成。broker 负责接收并处理请求，并对消息进行持久化。broker 通常分布在不同机器上，以实现高可用（HA）。

### 备份与高可用

实现高可用的另一种手段是备份（Replication），就是把同一份数据拷贝到多台机器上。这些相同的数据拷贝称为副本（Replica）。

副本数量是可配置的，分为 leader 和 follower。Producer 发来的消息总是写到 Leader replica,而 Consumer 总是从 Leader replica 读消息；Follower Replica 则是备胎，只是向 leader replica 发送请求，让 Leader 把最新消息发给它，来实现同步。

为什么 Kafka 不像 MySQL 那样允许 follower 对外提供读服务？因为 kafka 的 leader 副本已经均匀分布在不同机器上，已经起到了负载均衡的作用。

### 分区与伸缩性（Scalability）。

如果数据量太大，导致领导者积累太多数据以至于单台 broker 无法容纳了，咋办？这时候就需要分区（partitioning）。（在其他系统里面有时候被称为 sharding, region）。

Kafka 的分区机制是指将每个 Topic 分成多个分区，每个消息只会被发送到一个分区。Kafka 的分区编号是从 0 开始的，如果某个 Topic 有 100 个分区，那么它们的分区号就是 0 到 99。

副本是在分区这个层级上定义的。每条消息在分区中的位置信息由一个 Offset 来表征。

### 三层消息架构：

- 主题层，每个主题可以配置 M 个分区，每个分区可以有 N 个副本。
- 分区层，每个分区有 1 个 Leader 副本和 N-1 个 Follower 副本。follower 副本只做数据冗余。
- 消息层，分区中包含若干消息，每条消息的位移从 0 开始。

### Kafka Broker 是如何持久化数据的？

Kafka 使用消息日志（Log）来保存数据。一个日志就是磁盘上一个 Append-only 的文件（顺序写）。

Kafka 通过日志段（Log segment）机制来实现日志删除。在 Kafka 底层，一个日志被分成多个日志段，消息被追加到当前最新的 log segment 中。当一个 Log segment 被写满后，kafka 会自动切分出一个新的日志段，并将老的日志段封存起来。kafka 会定期检查老的 log segment 是否能被删除，从而实现磁盘空间回收的目的。

### Consumer Group

主要是为了提升消费端的吞吐，多个消费者同时消费。

假设组内某个实例挂掉了，Kafka 能自动检测到并把这个 Failed 实例之前负责的分区转移给其他活着的消费者。这就是 Kafka 中大名鼎鼎的“重平衡”（Rebalance）。嗯，其实既是大名鼎鼎，也是臭名昭著，因为由重平衡引发的消费者问题比比皆是。事实上，目前很多重平衡的 Bug 社区都无力解决。

每个消费者在消费消息的过程中必然需要有个字段记录它当前消费到了分区的哪个位置上，这个字段就是消费者位移（Consumer Offset）。注意，这和上面所说的位移完全不是一个概念。上面的“位移”表征的是分区内的消息位置，它是不变的，即一旦消息被成功写入到一个分区上，它的位移值就是固定的了。而消费者位移则不同，它可能是随时变化的，毕竟它是消费者消费进度的指示器嘛。另外每个消费者有着自己的消费者位移，因此一定要区分这两类位移的区别。我个人把消息在分区中的位移称为分区位移，而把消费者端的位移称为消费者位移。

### 生态

kafka 不单是一个消息引擎系统，而且是能够实现 Exactly-Once 处理语义的实时流处理平台。
你可能听说过 Apache Storm、Apache Spark Streaming 亦或是 Apache Flink，它们在大规模流处理领域可都是响当当的名字。令人高兴的是，Kafka 经过这么长时间不断的迭代，现在已经能够稍稍比肩这些框架了。我在这里使用了“稍稍”这个字眼，一方面想表达 Kafka 社区对于这些框架心存敬意；另一方面也想表达目前国内鲜有大厂将 Kafka 用于流处理的尴尬境地，毕竟 Kafka 是从消息引擎“半路出家”转型成流处理平台的，它在流处理方面的表现还需要经过时间的检验。
如果我们把视角从流处理平台扩展到流处理生态圈，Kafka 更是还有很长的路要走。前面我提到过 Kafka Streams 组件，正是它提供了 Kafka 实时处理流数据的能力。但是其实还有一个重要的组件我没有提及，那就是 Kafka Connect。
我们在评估流处理平台的时候，框架本身的性能、所提供操作算子（Operator）的丰富程度固然是重要的评判指标，但框架与上下游交互的能力也是非常重要的。能够与之进行数据传输的外部系统越多，围绕它打造的生态圈就越牢固，因而也就有更多的人愿意去使用它，从而形成正向反馈，不断地促进该生态圈的发展。就 Kafka 而言，Kafka Connect 通过一个个具体的连接器（Connector），串联起上下游的外部系统。
整个 Kafka 生态圈如下图所示。值得注意的是，这张图中的外部系统只是 Kafka Connect 组件支持的一部分而已。目前还有一个可喜的趋势是使用 Kafka Connect 组件的用户越来越多，相信在未来会有越来越多的人开发自己的连接器。

## 一些问题

解释流 API 的作用？一种允许应用程序充当流处理器的 API，它还使用一个或多个主题的输入流，并生成一个输出流到一个或多个输出主题，此外，有效地将输入流转换为输出流，我们称之为流 API。

连接器 API 的作用是什么？一个允许运行和构建可重用的生产者或消费者的 API，将 Kafka 主题连接到现有的应用程序或数据系统，我们称之为连接器 API。
ß
Log Anatomy：我们将日志视为分区。基本上，数据源将消息写入日志。其优点之一是，在任何时候，都有一个或多个消费者从他们选择的日志中读取数据。下面的图表显示，数据源正在写入一个日志，而用户正在以不同的偏移量读取该日志。

Kafka 是消息引擎系统，也是一个分布式流处理平台（distributed streaming platform）。
Linkedin 最开始有强烈的数据实时处理方面的需求，其内部的诸多子系统要执行很多类型数据的处理与分析，包括业务系统和应用程序监控，以及用户行为数据处理。
他们遇到的问题： 1.数据收集主要采用轮询方式，而轮询的世界间隔很难把控，导致数据正确性不足。 2.系统高度定制化，维护成本高。各个子系统都需要对接数据收集模块。
Kafka 之前的定位其实是一个分布式、分区化且带备份功能的提交日志 (Commit Log) 服务。
Kafka 在设计之初旨在提供三个方面特性： 提供一套 API 实现生产者和消费者。降低网络传输开销和磁盘存储开销。实现高伸缩性架构。

## 流处理平台

在大数据工程领域，Kafka 在承接上下游、串联数据流管道方面发挥了重要的作用：所有的数据几乎都要从一个系统流入 Kafka 然后再流向下游的另一个系统中。基于此，Kafka 社区于 0.10.0.0 版本正式推出了流处理组件 Kafka Streams，也正是从这个版本开始，Kafka 正式“变身”为分布式的流处理平台，而不仅仅是消息引擎系统了。今天 Apache Kafka 是和 Apache Storm、Apache Spark 和 Apache Flink 同等级的实时流处理平台。
诚然，目前国内对 Kafka 是流处理平台的认知还尚不普及，其核心的流处理组件 Kafka Streams 更是少有大厂在使用。但我们也欣喜地看到，随着在 Kafka 峰会上各路大神们的鼎力宣传，如今利用 Kafka 构建流处理平台的案例层出不穷，而了解并有意愿使用 Kafka Streams 的厂商也是越来越多。作为流处理平台，Kafka 的优势在哪里呢？
第一点是更容易实现端到端的正确性（Correctness）。Google 大神 Tyler 曾经说过，流处理要最终替代它的“兄弟”批处理需要具备两点核心优势：要实现正确性和提供能够推导时间的工具。实现正确性是流处理能够匹敌批处理的基石。正确性一直是批处理的强项，而实现正确性的基石则是要求框架能提供精确一次处理语义，即处理一条消息有且只有一次机会能够影响系统状态。目前主流的大数据流处理框架都宣称实现了精确一次处理语义，但这是有限定条件的，即它们只能实现框架内的精确一次处理语义，无法实现端到端的。
这是为什么呢？因为当这些框架与外部消息引擎系统结合使用时，它们无法影响到外部系统的处理语义，所以如果你搭建了一套环境使得 Spark 或 Flink 从 Kafka 读取消息之后进行有状态的数据计算，最后再写回 Kafka，那么你只能保证在 Spark 或 Flink 内部，这条消息对于状态的影响只有一次。但是计算结果有可能多次写入到 Kafka，因为它们不能控制 Kafka 的语义处理。相反地，Kafka 则不是这样，因为所有的数据流转和计算都在 Kafka 内部完成，故 Kafka 可以实现端到端的精确一次处理语义。
可能助力 Kafka 胜出的第二点是它自己对于流式计算的定位。官网上明确标识 Kafka Streams 是一个用于搭建实时流处理的客户端库而非是一个完整的功能系统。这就是说，你不能期望着 Kafka 提供类似于集群调度、弹性部署等开箱即用的运维特性，你需要自己选择适合的工具或系统来帮助 Kafka 流处理应用实现这些功能。
读到这你可能会说这怎么算是优点呢？坦率来说，这的确是一个“双刃剑”的设计，也是 Kafka 社区“剑走偏锋”不正面 PK 其他流计算框架的特意考量。大型公司的流处理平台一定是大规模部署的，因此具备集群调度功能以及灵活的部署方案是不可或缺的要素。但毕竟这世界上还存在着很多中小企业，它们的流处理数据量并不巨大，逻辑也并不复杂，部署几台或十几台机器足以应付。在这样的需求之下，搭建重量级的完整性平台实在是“杀鸡焉用牛刀”，而这正是 Kafka 流处理组件的用武之地。因此从这个角度来说，未来在流处理框架中，Kafka 应该是有一席之地的。

## 为啥 kafka 这么快

写入数据采用**顺序写入和 mmap**。

即便是顺序写入，硬盘的访问速度也比不了内存，所以 kafka 的数据并非实时写盘。mmap 内存映射文件让 kafka 得以以操作内存的方式操作文件，而数据由操作系统在适当的时候写入硬盘。这可以省去用户空间到内核空间复制的开销（read 会把数据先放到内核空间内存，然后再复制到用户空间内存）。

kafka 提供了一个参数 producer.type 来控制是不是主动 flush。如果 kafka 写入 mmap 后立即 flush 就叫 async,否则就是 async。mmap 在 java 中可以使用 mmapedbytebuffer 来实现。

消费数据时的**zero copy**。

一般从网络发送一个文件是，先由 DMA 复制到内核空间，然后复制到用户空间，然后从用户空间复制到内核空间，再从网卡发送出去。而 zero copy 则不需要到用户空间，直接从 DMA 的内核空间到 socket 的内核空间。

kafka 将所有消息都放在一个个文件中，消费者需要数据的时候直接把文件发送出去。比如十万消息组合在一起是 10M 数据，发送出去可能只要 1 秒。当然，实际上不是真的发整个文件，具体可以参照系统调用 sendfile。

## 存储

每个 topic 包含若干个 partition,partition 分布在不同的 broker 上，每个 partition 是一个文件夹，每个文件夹里面有很多 segment 文件。segment file 分为 index 文件和 data 文件，成对出现。

segment index file 采取稀疏索引存储方式，它减少索引文件大小，通过 mmap 可以直接内存操作，稀疏索引为数据文件的每个对应 message 设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。

## 事件驱动设计（EDA）

使用 Broker 方式，服务间通过交换消息来完成交流和整个流程的驱动。比如下单服务通知订单服务有订单要处理，而订单服务生成订单后发出通知，库存服务和支付服务得通知后，一边占住库存，一边让用户支付，等用户支付完后通知配送服务进行商品配送。
每个服务都是“自包含”的。也就是没有和其他服务产生依赖。而我们通过一系列消息通道把整个流程给串联起来。
EDA 的好处：

1. 服务间的依赖没了，服务间是平等的，每个服务都可高度重用并可被替换。
2. 服务的开发，测试，运维，以及故障处理都是高度隔离的。
3. 服务间不会相互 block。
4. 在服务间增加一些 adapter（比如日志，认证，版本，限流，降级，熔断）比较容易。
5. 服务间的吞吐也被解开了，各个服务按照自己的 pace 来处理。
   EDA 的坏处：
6. 业务流程不再那么明显。整个架构变得比较复杂。
7. 事件可能会乱序，从而引发一些 bug。解决这个问题需要很好地管理一个状态机的控制。
8. 事务处理变得复杂。
   异步通信的设计重点
9. broker 要设计成高可用的，不丢消息的。你的设计也最好不依赖于消息的顺序。
10. 异步通信会导致业务处理流程不直观。所以在 broker 上需要有相关的服务消息跟踪机制，否则出现问题后不容易调试。
11. 服务间只通过消息交互，业务最好有一个总控放来管理，总控方维护一个业务流程的状态变迁逻辑，以便在 bug 出现后知道业务处理到哪一步（这样的设计常见于银行的对账程序，银行系统会有大量的外部系统通讯，比如跨行的交易、跨企业的交易，等等。所以，为了保证整体数据的一致性，或是避免漏处理及处理错的交易，需要有对账系统，这其实就是那个总控，这也是为什么银行有的交易是 T+1（隔天结算），就是因为要对个账，确保数据是对的。）
12. 消息传递中，可能有的业务逻辑会有 ACK 机制（比如：A 服务发出一个消息之后，开始等待处理方的 ACK，如果等不到的话，就需要做重传。此时，需要处理方有幂等的处理，即同一件消息无论收到多少次都只处理一次。）

## 异步任务处理

首先有一个前台系统，将用户请求都记录下来（有点像日志）。这种方式的效率很高，我们可以给用户返回“收到请求，正在处理中”。
然后我们有一个任务处理系统真正处理这些请求。这里就需要一个任务派发器，存在 push 和 pull 两种模型。
Push 模型是由调度者下发任务，而 Pull 模型是由调度者来拉取任务。有时候我们可以结合这两种模式使用。push 端可以做一些简单的任务调度，比如把若干订单合并成一个订单，或者把一个订单拆分成若干订单；而 Pull 端则拉取 push 端发出的异步消息来处理任务。

#### Event Sourcing 模式

如果我们记录了所有的收支记录，我完全不需要记录余额，只需要回放一下所有的收支记录。这样系统就变得很简单，我们只需要增加新的事件日志，而不用修改数据的最终状态。
关于 event sourcing 一般会和 CQRS 一起提，可以参考这个项目 https://github.com/cer/event-sourcing-examples

#### 异步处理的分布式事务

如果需要强一致性，在业务层面只能做 2PC,在数据层面要用 paxos 或者 raft。但是现实生活中很多场景不需要强一致。比如你去星巴克买咖啡，都是先结账，拿小票，凭小票领咖啡。也就是说，我们需要一个交易凭证。
关键点：1.凭证的保存很重要，不能丢；2.凭证的处理必须是幂等的；3.如果事务完成不了，要有补偿机制。

#### 关键设计

任务处理方需要在完成任务后回传状态（成功，失败），任务发起方也要有一个定时任务检查那些超时没有完成的任务，需要能够支持幂等操作。
需要监控任务队列大小。
需要有补偿机制。
