发布订阅的对象是主题 （Topic）。发布消息（Record）的是生产者（producer），订阅主题消息的是消费者（consumer），生产者和消费者都是客户端（clients）。

kafka 服务端由 broker 服务进程组成。broker 负责接收并处理请求，并对消息进行持久化。broker 通常分布在不同机器上，以实现高可用（HA）。

### 备份与高可用(HA)

副本数量是可配置的，分为 leader 和 follower。Producer 发来的消息总是写到 Leader replica,而 Consumer 总是从 Leader replica 读消息；Follower Replica 则是备胎，只是向 leader replica 发送请求，让 Leader 把最新消息发给它，来实现同步。副本备份有助于高可用。

为什么 Kafka 不像 MySQL 那样允许 follower 对外提供读服务？因为 kafka 的 leader 副本已经均匀分布在不同机器上，已经起到了负载均衡的作用。

### 分区与伸缩性（Scalability）。

分区（partitioning）。（在其他系统里面有时候被称为 sharding, region）。

Kafka 的分区机制是指将每个 Topic 分成多个分区，每个消息只会被发送到一个分区。Kafka 的分区编号是从 0 开始的，如果某个 Topic 有 100 个分区，那么它们的分区号就是 0 到 99。副本是在分区这个层级上定义的。

### 三层消息架构：

- 主题层，每个主题可以配置 M 个分区，每个分区可以有 N 个副本。
- 分区层，每个分区有 1 个 Leader 副本和 N-1 个 Follower 副本。follower 副本只做数据冗余。
- 消息层，分区中包含若干消息，每条消息的位移从 0 开始。

### Kafka 的最初设计

Kafka 是消息引擎系统，也是一个分布式流处理平台。Linkedin 最开始有强烈的数据实时处理方面的需求，其内部的诸多子系统要执行很多类型数据的处理与分析，包括业务系统和应用程序监控，以及用户行为数据处理。他们遇到的问题： 1.数据收集主要采用轮询方式，而轮询的间隔很难把控，导致数据正确性不足。 2.系统高度定制化，维护成本高。各个子系统都需要对接数据收集模块。

Kafka 之前的定位其实是一个分布式、分区化且带备份功能的提交日志 (Commit Log) 服务。其设计之初旨在提供三个方面特性：**提供一套 API 实现生产者和消费者。降低网络传输开销和磁盘存储开销。实现高伸缩性架构。**

### 为啥 kafka 这么快

**顺序写入和 mmap**。数据并非实时写盘，mmap 以操作内存的方式操作文件，而数据由操作系统在适当的时候写入硬盘。这可以省去用户空间到内核空间复制的开销。kafka 提供参数 producer.type 来控制是不是主动 flush。mmap 在 java 中可以使用 mmapedbytebuffer 来实现。

消费数据时的**zero copy**。一般从网络发送文件，先由 DMA 复制到内核空间，再复制到用户空间，然后从用户空间复制到内核空间，再从网卡发送出去。而 zero copy 则不需要到用户空间，直接从 DMA 的内核空间到 socket 的内核空间。

kafka 将所有消息都放在一个个文件中，消费者需要数据的时候直接把文件发送出去。比如十万消息组合在一起是 10M 数据，发送出去可能只要 1 秒。当然，实际上不是真的发整个文件，具体可以参照系统调用 sendfile。

### Kafka Broker 是如何持久化数据的？

- 每个 topic 包含若干个 partition，分布在不同的 broker 上。
- 每个 partition 是个文件夹，里面有很多 segment 文件。消息被追加到当前最新的段中。
- segment file 分为 index 文件和 data 文件，成对出现。
- segment index file 采取稀疏索引存储方式，来减少索引文件大小。
- 稀疏索引比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。
- 当一个 Log segment 被写满后，会自动切分出一个新日志段，并将老的日志段封存起来。
- 定期检查老的 log segment 是否能被删除，从而实现磁盘空间回收的目的。

### 生态

kafka 不单是一个消息引擎系统，而且是能够实现 Exactly-Once 处理语义的实时流处理平台。

如果我们把视角从流处理平台扩展到流处理生态圈，Kafka 更是还有很长的路要走。前面我提到过 Kafka Streams 组件，正是它提供了 Kafka 实时处理流数据的能力。但是其实还有一个重要的组件我没有提及，那就是 Kafka Connect。

我们在评估流处理平台的时候，框架本身的性能、所提供操作算子（Operator）的丰富程度固然是重要的评判指标，但框架与上下游交互的能力也是非常重要的。能够与之进行数据传输的外部系统越多，围绕它打造的生态圈就越牢固，因而也就有更多的人愿意去使用它，从而形成正向反馈，不断地促进该生态圈的发展。就 Kafka 而言，Kafka Connect 通过一个个具体的连接器（Connector），串联起上下游的外部系统。

### 一些问题

**流 API 的作用？**一种允许应用程序充当流处理器的 API，它还使用一个或多个主题的输入流，并生成一个输出流到一个或多个输出主题，此外，有效地将输入流转换为输出流，我们称之为流 API。

**连接器 API 的作用？**一个允许运行和构建可重用的生产者或消费者的 API，将 Kafka 主题连接到现有的应用程序或数据系统，我们称之为连接器 API。

### 流处理平台

目前 Kafka Streams 少有大厂在使用。但如今利用 Kafka 构建流处理平台的案例层出不穷，而了解并有意愿使用 Kafka Streams 的厂商也是越来越多。作为流处理平台，Kafka 的优势在哪里呢？

第一点是更容易实现端到端的正确性（Correctness）。Google 大神 Tyler 曾经说过，流处理要最终替代它的“兄弟”批处理需要具备两点核心优势：要实现正确性和提供能够推导时间的工具。实现正确性是流处理能够匹敌批处理的基石。正确性一直是批处理的强项，而实现正确性的基石则是要求框架能提供精确一次处理语义，即处理一条消息有且只有一次机会能够影响系统状态。目前主流的大数据流处理框架都宣称实现了精确一次处理语义，但这是有限定条件的，即它们只能实现框架内的精确一次处理语义，无法实现端到端的。

这是为什么呢？因为当这些框架与外部消息引擎系统结合使用时，它们无法影响到外部系统的处理语义，所以如果你搭建了一套环境使得 Spark 或 Flink 从 Kafka 读取消息之后进行有状态的数据计算，最后再写回 Kafka，那么你只能保证在 Spark 或 Flink 内部，这条消息对于状态的影响只有一次。但是计算结果有可能多次写入到 Kafka，因为它们不能控制 Kafka 的语义处理。相反地，Kafka 则不是这样，因为所有的数据流转和计算都在 Kafka 内部完成，故 Kafka 可以实现端到端的精确一次处理语义。

可能助力 Kafka 胜出的第二点是它自己对于流式计算的定位。官网上明确标识 Kafka Streams 是一个用于搭建实时流处理的客户端库而非是一个完整的功能系统。这就是说，你不能期望着 Kafka 提供类似于集群调度、弹性部署等开箱即用的运维特性，你需要自己选择适合的工具或系统来帮助 Kafka 流处理应用实现这些功能。

读到这你可能会说这怎么算是优点呢？坦率来说，这的确是一个“双刃剑”的设计，也是 Kafka 社区“剑走偏锋”不正面 PK 其他流计算框架的特意考量。大型公司的流处理平台一定是大规模部署的，因此具备集群调度功能以及灵活的部署方案是不可或缺的要素。但毕竟这世界上还存在着很多中小企业，它们的流处理数据量并不巨大，逻辑也并不复杂，部署几台或十几台机器足以应付。在这样的需求之下，搭建重量级的完整性平台实在是“杀鸡焉用牛刀”，而这正是 Kafka 流处理组件的用武之地。因此从这个角度来说，未来在流处理框架中，Kafka 应该是有一席之地的。

## 事件驱动设计（EDA）

比如下单服务通知订单服务有订单要处理，而订单服务生成订单后发出通知，库存服务和支付服务得通知后，一边占住库存，一边让用户支付，等用户支付完后通知配送服务进行商品配送。每个服务都是“自包含”的。也就是没有和其他服务产生依赖。而我们通过一系列消息通道把整个流程给串联起来。

EDA 的好处：

- 服务间的依赖没了，每个服务都可高度重用并可被替换。
- 服务的开发，测试，运维，以及故障处理都是高度隔离的。
- 服务间不会相互 block。
- 服务间增加一些 adapter（如日志，认证，版本，限流，降级，熔断）较容易。
- 服务间的吞吐也被解开了，各个服务按照自己的 pace 来处理。

EDA 的坏处：

- 业务流程不再那么明显。整个架构变得比较复杂。
- 事件可能会乱序，从而引发一些 bug。需要很好地管理一个状态机的控制。
- 事务处理变得复杂。

服务间只通过消息交互，业务最好有一个总控方来管理，总控方维护一个业务流程的状态变迁逻辑，以便在 bug 出现后知道业务处理到哪一步（这样的设计常见于银行的对账程序，银行系统会有大量的外部系统通讯，比如跨行的交易、跨企业的交易，等等。所以，为了保证整体数据的一致性，或是避免漏处理及处理错的交易，需要有对账系统，这其实就是那个总控，这也是为什么银行有的交易是 T+1（隔天结算），就是因为要对个账，确保数据是对的。）

### 异步任务处理

- 首先有一个前台系统，将用户请求都记录下来（有点像日志）。
- 由任务处理系统处理请求，需要一个任务派发器，存在 push 和 pull 两种模型。
- Push 模型是由调度者下发任务，而 Pull 模型是由调度者来拉取任务。
- 有时候我们可以结合这两种模式使用。push 端可以做一些简单的任务调度，比如把若干订单合并成一个订单，或者把一个订单拆分成若干订单；而 Pull 端则拉取 push 端发出的异步消息来处理任务。

### Event Sourcing 模式

如果我们记录了所有的收支记录，我完全不需要记录余额，只需要回放一下所有的收支记录。这样系统就变得很简单，我们只需要增加新的事件日志，而不用修改数据的最终状态。关于 event sourcing 一般会和 CQRS 一起提，可以参考这个项目 https://github.com/cer/event-sourcing-examples

### 异步处理的分布式事务

如果需要强一致性，在业务层面只能做 2PC,在数据层面要用 paxos 或者 raft。但是现实生活中很多场景不需要强一致。比如你去星巴克买咖啡，都是先结账，拿小票，凭小票领咖啡。也就是说，我们需要一个交易凭证。

关键点：1.凭证的保存很重要，不能丢；2.凭证的处理必须是幂等的；3.如果事务完成不了，要有补偿机制。

### 消息积压怎么解决

- 扩容，确保消息不丢。
- 检查消费者消费是否出了问题。
- 临时增加消费者数量，提高消费速度。
- 生产者限流？

### kafka 性能调优

- 优化目标：高吞吐，低延迟。
- 文件系统建议选择 ext4 或 XFS，尤其是 XFS。
- swap 建议设置小一点，比如设为 1。sudo sysctl vm.swappiness=1，或者直接修改 /etc/sysctl.conf 并重启。
- ulimit -n 要改大。
- vm.max_map_count 要改大，修改 /etc/sysctl.conf 文件，增加 vm.max_map_count=655360。
- 给 Kafka 预留的页缓存越大越好，至少要容纳一个日志段的大小（log.segment.bytes 的值。该参数的默认值是 1GB）。
- 预留出一个日志段大小，至少能保证 Kafka 可以将整个日志段全部放入页缓存，避免消费时昂贵的物理磁盘 I/O 操作。
- 堆大小设置为 6 到 8 G。
- 建议使用 G1 GC。要竭力避免 full gc。
- 不要频繁创建 Producer 和 Consumer 对象实例，尽量复用他们。
- 用完及时关闭，避免内存泄露。
- Producer 是线程安全的，可以在多个线程中复用；而 Consumer 不是线程安全的，应当用任务队列加工作线程池的方式。

##### 调优吞吐量

- Broker 端 num.replica.fetchers 表示的是 Follower 副本用多少个线程来拉取消息，默认 1 个，可适当调大。
- Producer 端改善吞吐量，通常的标配是增加消息批次的大小以及批次缓存时间，即 batch.size 和 linger.ms。
- 默认 16KB 的消息批次大小太小。假设你的消息体大小是 1KB，默认一个消息批次也就大约 16 条消息。
- 最好把压缩算法也配置上，以减少网络 I/O 传输量，从而间接提升吞吐量。当前适配最好的压缩算法是 LZ4 和 zstd。
- 对吞吐量要求高时，最好不要设置 acks=all 以及开启重试。

##### 调优延迟

- 在 Broker 端，我们依然要增加 num.replica.fetchers 值以加快 Follower 副本的拉取速度，减少整个消息处理的延时。
- 在 Producer 端，我们希望消息尽快地被发送出去，因此不要有过多停留，所以必须设置 linger.ms=0，同时不要启用压缩。因为压缩操作本身要消耗 CPU 时间，会增加消息发送的延时。另外，最好不要设置 acks=all。我们刚刚在前面说过，Follower 副本同步往往是降低 Producer 端吞吐量和增加延时的首要原因。
- 在 Consumer 端，我们保持 fetch.min.bytes=1 即可，也就是说，只要 Broker 端有能返回的数据，立即令其返回给 Consumer，缩短 Consumer 消费延时。
