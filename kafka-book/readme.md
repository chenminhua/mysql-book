## 三层消息架构：

- 主题层，每个主题可以配置 M 个分区，每个分区可以有 N 个副本。
- 分区层，每个分区有 1 个 Leader 副本和 N-1 个 Follower 副本。follower 副本只做数据冗余。
- 消息层，分区中包含若干消息，每条消息的位移从 0 开始。

备份与高可用(HA)

- 副本数量是可配置的，分为 leader 和 follower。副本备份有助于高可用。
- Producer 发来的消息总是写到 Leader replica,而 Consumer 总是从 Leader replica 读消息；
- Follower Replica 则是备胎，只是向 leader replica 发送请求，让 Leader 把最新消息发给它，来实现同步。
- 为什么不允许 follower 对外提供读服务？因为 leader 副本已经均匀分布在不同机器上，起到了负载均衡的作用。

分区与伸缩性（Scalability）。

- 分区（partitioning）。（在其他系统里面有时候被称为 sharding, region）。
- Kafka 的分区机制是指将每个 Topic 分成多个分区，每个消息只会被发送到一个分区。
- 如果某个 Topic 有 100 个分区，那么它们的分区号就是 0 到 99。副本是在分区这个层级上定义的。

Kafka Broker 是如何持久化数据的？

- 每个 topic 包含若干个 partition，分布在不同的 broker 上。
- 每个 partition 是个文件夹，里面有很多 segment 文件。消息被追加到当前最新的段中。
- segment file 分为 index 文件和 data 文件，成对出现。
- segment index file 采取稀疏索引存储方式，来减少索引文件大小。
- 稀疏索引比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。
- 当一个 Log segment 被写满后，会自动切分出一个新日志段，并将老的日志段封存起来。
- 定期检查老的 log segment 是否能被删除，从而实现磁盘空间回收的目的。

## 为啥 kafka 这么快

- **顺序写入和 mmap**。数据并非实时写盘，mmap 以操作内存的方式操作文件，而数据由操作系统在适当的时候写入硬盘。
- kafka 提供参数 producer.type 来控制是不是主动 flush。mmap 在 java 中可以使用 mmapedbytebuffer 来实现。
- 消费数据时的**zero copy**，不需要到用户空间，直接从 DMA 的内核空间到 socket 的内核空间。
- kafka 将所有消息都放在一个个文件中，消费者需要数据的时候直接把文件发送出去。比如十万消息组合在一起是 10M 数据，发送出去可能只要 1 秒。当然，实际上不是真的发整个文件，具体可以参照系统调用 sendfile。

## 使用场景

#### 事件驱动设计（EDA）

比如下单服务通知订单服务有订单要处理，而订单服务生成订单后发出通知，库存服务和支付服务得通知后，一边占住库存，一边让用户支付，等用户支付完后通知配送服务进行商品配送。每个服务都是“自包含”的。也就是没有和其他服务产生依赖。而我们通过一系列消息通道把整个流程给串联起来。

EDA 的好处：

- 服务间的依赖没了，每个服务都可高度重用并可被替换。
- 服务的开发，测试，运维，以及故障处理都是高度隔离的。
- 服务间不会相互 block。
- 服务间增加一些 adapter（如日志，认证，版本，限流，降级，熔断）较容易。
- 服务间的吞吐也被解开了，各个服务按照自己的 pace 来处理。

EDA 的坏处：

- 业务流程不再那么明显。整个架构变得比较复杂。
- 事件可能会乱序，从而引发一些 bug。需要很好地管理一个状态机的控制。
- 事务处理变得复杂。

服务间只通过消息交互，业务最好有一个总控方来管理，总控方维护一个业务流程的状态变迁逻辑，以便在 bug 出现后知道业务处理到哪一步（这样的设计常见于银行的对账程序，银行系统会有大量的外部系统通讯，比如跨行的交易、跨企业的交易，等等。所以，为了保证整体数据的一致性，或是避免漏处理及处理错的交易，需要有对账系统，这其实就是那个总控，这也是为什么银行有的交易是 T+1（隔天结算），就是因为要对个账，确保数据是对的。）

#### 异步任务处理

- 首先有一个前台系统，将用户请求都记录下来（有点像日志）。
- 由任务处理系统处理请求，需要一个任务派发器，存在 push 和 pull 两种模型。
- Push 模型是由调度者下发任务，而 Pull 模型是由调度者来拉取任务。
- 有时候我们可以结合这两种模式使用。push 端可以做一些简单的任务调度，比如把若干订单合并成一个订单，或者把一个订单拆分成若干订单；而 Pull 端则拉取 push 端发出的异步消息来处理任务。

#### Event Sourcing 模式

如果我们记录了所有的收支记录，我完全不需要记录余额，只需要回放一下所有的收支记录。这样系统就变得很简单，我们只需要增加新的事件日志，而不用修改数据的最终状态。关于 event sourcing 一般会和 CQRS 一起提，可以参考这个项目 https://github.com/cer/event-sourcing-examples

#### 异步处理的分布式事务

如果需要强一致性，在业务层面只能做 2PC,在数据层面要用 paxos 或者 raft。但是现实生活中很多场景不需要强一致。比如你去星巴克买咖啡，都是先结账，拿小票，凭小票领咖啡。也就是说，我们需要一个交易凭证。

关键点：1.凭证的保存很重要，不能丢；2.凭证的处理必须是幂等的；3.如果事务完成不了，要有补偿机制。

## kafka 性能调优

- 优化目标：高吞吐，低延迟。
- 文件系统建议选择 ext4 或 XFS，尤其是 XFS。
- swap 建议设置小一点，比如设为 1。sudo sysctl vm.swappiness=1，或者直接修改 /etc/sysctl.conf 并重启。
- ulimit -n 要改大。
- vm.max_map_count 要改大，修改 /etc/sysctl.conf 文件，增加 vm.max_map_count=655360。
- 给 Kafka 预留的页缓存越大越好，至少要容纳一个日志段的大小（log.segment.bytes 的值。该参数的默认值是 1GB）。
- 预留出一个日志段大小，至少能保证 Kafka 可以将整个日志段全部放入页缓存，避免消费时昂贵的物理磁盘 I/O 操作。
- 堆大小设置为 6 到 8 G。
- 建议使用 G1 GC。要竭力避免 full gc。
- 不要频繁创建 Producer 和 Consumer 对象实例，尽量复用他们。
- 用完及时关闭，避免内存泄露。
- Producer 是线程安全的，可以在多个线程中复用；而 Consumer 不是线程安全的，应当用任务队列加工作线程池的方式。

#### 调优吞吐量

- Broker 端 num.replica.fetchers 表示的是 Follower 副本用多少个线程来拉取消息，默认 1 个，可适当调大。
- Producer 端改善吞吐量，通常的标配是增加消息批次的大小以及批次缓存时间，即 batch.size 和 linger.ms。
- 默认 16KB 的消息批次大小太小。假设你的消息体大小是 1KB，默认一个消息批次也就大约 16 条消息。
- 最好把压缩算法也配置上，以减少网络 I/O 传输量，从而间接提升吞吐量。当前适配最好的压缩算法是 LZ4 和 zstd。
- 对吞吐量要求高时，最好不要设置 acks=all 以及开启重试。

#### 调优延迟

- 在 Broker 端，我们依然要增加 num.replica.fetchers 值以加快 Follower 副本的拉取速度，减少整个消息处理的延时。
- 在 Producer 端，我们希望消息尽快地被发送出去，因此不要有过多停留，所以必须设置 linger.ms=0，同时不要启用压缩。因为压缩操作本身要消耗 CPU 时间，会增加消息发送的延时。另外，最好不要设置 acks=all。我们刚刚在前面说过，Follower 副本同步往往是降低 Producer 端吞吐量和增加延时的首要原因。
- 在 Consumer 端，我们保持 fetch.min.bytes=1 即可，也就是说，只要 Broker 端有能返回的数据，立即令其返回给 Consumer，缩短 Consumer 消费延时。

#### 消息积压怎么解决

- 扩容，确保消息不丢。
- 检查消费者消费是否出了问题。
- 临时增加消费者数量，提高消费速度。
- 生产者限流？

## 生态

- Kafka 之前的定位其实是一个分布式、分区化且带备份功能的提交日志 (Commit Log) 服务。其设计之初旨在提供三个方面特性：**提供一套 API 实现生产者和消费者。降低网络传输开销和磁盘存储开销。实现高伸缩性架构。**
- kafka 不单是一个消息引擎系统，而且是能够实现 Exactly-Once 处理语义的实时流处理平台。
- Kafka Streams 提供了 Kafka 实时处理流数据的能力。
- **流 API 的作用？**允许应用程序充当流处理器，它还使用一个或多个主题的输入流，并生成一个输出流到一个或多个输出主题。
- Kafka Connect 用来串联起上下游的外部系统。
- **连接器 API 的作用？**允许构建可重用的生产者或消费者，将 Kafka 主题连接到现有的应用程序或数据系统。

#### 流处理平台

第一点是更容易实现端到端的正确性（Correctness）。Google 大神 Tyler 曾经说过，流处理要最终替代它的“兄弟”批处理需要具备两点核心优势：要实现正确性和提供能够推导时间的工具。实现正确性是流处理能够匹敌批处理的基石。正确性一直是批处理的强项，而实现正确性的基石则是要求框架能提供精确一次处理语义，即处理一条消息有且只有一次机会能够影响系统状态。目前主流的大数据流处理框架都宣称实现了精确一次处理语义，但这是有限定条件的，即它们只能实现框架内的精确一次处理语义，无法实现端到端的。

这是为什么呢？因为当这些框架与外部消息引擎系统结合使用时，它们无法影响到外部系统的处理语义，所以如果你搭建了一套环境使得 Spark 或 Flink 从 Kafka 读取消息之后进行有状态的数据计算，最后再写回 Kafka，那么你只能保证在 Spark 或 Flink 内部，这条消息对于状态的影响只有一次。但是计算结果有可能多次写入到 Kafka，因为它们不能控制 Kafka 的语义处理。相反地，Kafka 则不是这样，因为所有的数据流转和计算都在 Kafka 内部完成，故 Kafka 可以实现端到端的精确一次处理语义。

可能助力 Kafka 胜出的第二点是它自己对于流式计算的定位。官网上明确标识 Kafka Streams 是一个用于搭建实时流处理的客户端库而非是一个完整的功能系统。这就是说，你不能期望着 Kafka 提供类似于集群调度、弹性部署等开箱即用的运维特性，你需要自己选择适合的工具或系统来帮助 Kafka 流处理应用实现这些功能。

读到这你可能会说这怎么算是优点呢？坦率来说，这的确是一个“双刃剑”的设计，也是 Kafka 社区“剑走偏锋”不正面 PK 其他流计算框架的特意考量。大型公司的流处理平台一定是大规模部署的，因此具备集群调度功能以及灵活的部署方案是不可或缺的要素。但毕竟这世界上还存在着很多中小企业，它们的流处理数据量并不巨大，逻辑也并不复杂，部署几台或十几台机器足以应付。在这样的需求之下，搭建重量级的完整性平台实在是“杀鸡焉用牛刀”，而这正是 Kafka 流处理组件的用武之地。因此从这个角度来说，未来在流处理框架中，Kafka 应该是有一席之地的。

# 部署环境

使用普通磁盘组成存储空间即可。使用机械磁盘完全能够胜任 Kafka 线上环境。假设业务每天发 1 亿条消息，两个副本，保存两周，平均每条消息占用磁盘 1KB。估算一下总的空间大小是 200GB 每天。如果预留 10%的空间给消息之外的其他数据，则是 220G 每天。乘以 14，在乘以 0.75 的压缩比（假设），大概需要 2.25TB。

对于 kafka 这种通过网络进行大量数据传输的框架而言，带宽很容易成为瓶颈。带宽不足导致的性能问题占一半以上。以 1Gbps 的千兆网络为例，假设你现在的业务目标是在 1 小时内处理 1TB 的业务数据，请问你需要多少台 kafka 服务器？由于带宽是 1Gbps，即每秒处理 1G 数据，假设 kafka 能用到 70%的带宽资源，也就是 700Mbps。你还不能让 kafka 常规性地使用这么大带宽，通常只有三分之一，大约 240Mbps。这样，我们大概可以算出一小时处理 1TB 数据需要 10 台 kafka。如果你希望数据一共有三个副本，则是 30 台。

### 关键配置

- 静态参数：必须在配置文件 server.properties 中设置的参数，重启才生效。
- 主题级别参数的设置则有所不同，通过 kafka-configs 命令来修改它们。
- JVM 和操作系统级别参数的设置方法比较通用化。

### Broker 存储相关

- log.dirs：这是非常重要的参数，指定了 Broker 需要使用的若干个文件目录路径。
- log.dir：注意这是 dir，结尾没有 s，说明它只能表示单个路径，它是补充上一个参数用的。
- 只设置 log.dirs 就好，不要设置 log.dir。线上生产环境中一定要为 log.dirs 配置多个路径。

### zk 相关

Kafka 与 Zk 相关的最重要的参数当属 **zookeeper.connect**。比如 zk1:2181,zk2:2181,zk3:2181。2181 是 ZooKeeper 的默认端口。如果你有两套 Kafka 集群，假设分别叫它们 kafka1 和 kafka2，那么两套集群的 zookeeper.connect 参数可以这样指定：zk1:2181,zk2:2181,zk3:2181/kafka1 和 zk1:2181,zk2:2181,zk3:2181/kafka2。切记 chroot 只需要写一次，而且是加到最后的。

### broker 连接相关

- listeners：监听器，告诉外部连接者通过什么协议访问指定主机名和端口开放的 Kafka 服务。
- advertised.listeners：这组监听器是 Broker 用于对外发布的。
- host.name/port：过期参数了，压根不要为它们指定值。

主机名这个设置中最好全部使用主机名，即 Broker 端和 Client 端应用配置中全部填写主机名。

### topic 管理

- auto.create.topics.enable：是否允许自动创建 Topic。最好是 false。
- unclean.leader.election.enable：是否允许 Unclean Leader 选举。最好是 false。
- auto.leader.rebalance.enable：是否允许定期进行 Leader 选举。最好是 false。

### 数据留存方面

- log.retention.{hours|minutes|ms}：都是控制一条消息数据被保存多长时间。通常设置 hours，比如 log.retention.hours=168 表示保存 7 天的数据，自动删除 7 天前数据。
- log.retention.bytes：指定 Broker 为消息保存的总磁盘容量大小。默认 -1（不设限）。
- message.max.bytes：控制 Broker 能接收的最大消息大小。默认 1000012 太少了。

### topic 级别参数

- retention.ms：规定了该 Topic 消息被保存的时长。默认是 7 天，即该 Topic 只保存最近 7 天的消息。一旦设置了这个值，它会覆盖掉 Broker 端的全局参数值。
- retention.bytes：规定了要为该 Topic 预留多大的磁盘空间。和全局参数作用相似，这个值通常在多租户的 Kafka 集群中会有用武之地。当前默认值是 -1，表示可以无限使用磁盘空间。

```sh
# 创建topic
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic transaction --partitions 1 --replication-factor 1 --config retention.ms=15552000000 --config max.message.bytes=5242880

# 修改topic设置
bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name transaction --alter --add-config max.message.bytes=10485760
```

### JVM 参数

如果 Broker 所在机器的 CPU 资源非常充裕，建议使用 CMS 收集器。启用方法是指定-XX:+UseCurrentMarkSweepGC。否则，使用吞吐量收集器。开启方法是指定-XX:+UseParallelGC。 **如果你已经在使用 Java 8 了，那么就用默认的 G1 收集器就好了**。在没有任何调优的情况下，G1 表现得要比 CMS 出色，主要体现在更少的 Full GC，需要调整的参数更少等，所以使用 G1 就好了。

```sh
$> export KAFKA_HEAP_OPTS=--Xms6g  --Xmx6g
$> export KAFKA_JVM_PERFORMANCE_OPTS= -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true
$> bin/kafka-server-start.sh config/server.properties
```

一个无脑通用的建议：将 JVM 堆设置成 6GB，这是目前业界比较公认的一个合理值。默认的 1GB 有点太小。毕竟 Kafka Broker 在与客户端进行交互时会在 JVM 堆上创建大量的 ByteBuffer 实例，Heap Size 不能太小。

### 操作系统参数

- 首先是 ulimit -n。通常将它设置成一个超大的值是合理，比如 ulimit -n 1000000。
- 文件系统。如 ext3、ext4 或 XFS 这样的日志型文件系统。生产环境最好还是使用 XFS。
- swap 的调优。建议将 swappniess 配置成一个接近 0 但不为 0 的值，比如 1。
- 提交时间或者说是 Flush 落盘时间。向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期将页缓存上的“脏”数据落盘到物理磁盘上。这个定期就是由提交时间来确定的，默认是 5 秒。一般情况下我们会认为这个时间太频繁了，可以适当地增加提交间隔来降低物理磁盘的写操作。当然你可能会有这样的疑问：如果在页缓存中的数据在写入到磁盘前机器宕机了，那岂不是数据就丢失了。的确，这种情况数据确实就丢失了，但鉴于 Kafka 在软件层面已经提供了多副本的冗余机制，因此这里稍微拉大提交间隔去换取性能还是一个合理的做法。

# 无消息丢失配置

**Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证.** 当 Kafka 的若干个 Broker 成功地接收到一条消息并写入到日志文件后，它们会告诉生产者程序这条消息已成功提交。此时，这条消息在 Kafka 看来就正式变为“已提交”消息了。那为什么是若干个 Broker 呢？这取决于你对“已提交”的定义。你可以选择只要有一个 Broker 成功保存该消息就算是已提交，也可以是令所有 Broker 都成功保存该消息才算是已提交。不论哪种情况，Kafka 只对已提交的消息做持久化保证这件事情是不变的。

### 消息丢失案例

案例 1：Producer 程序丢失消息。这应该算是被抱怨最多的数据丢失场景了。Producer 是异步发送消息的，调用 producer.send(msg) 这个 API 是 fire and forget 的，通常会立即返回，但不能认为消息发送已完成。

解决此问题的方法非常简单：**使用 producer.send(msg, callback)，不要使用 producer.send(msg)**。一旦出现消息提交失败的情况，你就可以有针对性地进行处理。

举例来说，如果是因为那些瞬时错误，那么仅仅让 Producer 重试就可以了；如果是消息不合格造成的，那么可以调整消息格式后再次发送。总之，处理发送失败的责任在 Producer 端而非 Broker 端。你可能会问，发送失败真的没可能是由 Broker 端的问题造成的吗？当然可能！如果你所有的 Broker 都宕机了，那么无论 Producer 端怎么重试都会失败的，此时你要做的是赶快处理 Broker 端的问题。但之前说的核心论据在这里依然是成立的：Kafka 依然不认为这条消息属于已提交消息，故对它不做任何持久化保证。

案例 2 ：Consumer 程序丢失数据。解决办法很简单：**先消费消息，再更新位移**。这种方式带来的问题是消息的重复处理。如果是多线程异步处理消费消息，Consumer 程序不要开启自动提交位移，而是要应用程序手动提交位移。**单个 Consumer 程序使用多线程来消费消息说起来容易，写成代码却异常困难，因为你很难正确地处理位移的更新，也就是说避免无消费消息丢失很简单，但极易出现消息被消费了多次的情况**。

### 最佳实践

- 使用 producer.send(msg, callback)。要有回调。
- 设置 acks = all。生产者参数。表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。
- 设置 retries（生产者参数）为一个较大的值。当出现网络的瞬时抖动时，自动重试发送消息。
- 设置 unclean.leader.election.enable = false。（Broker 参数，不让不干净的节点参加竞选）
- 设置 replication.factor >= 3。Broker 端的参数。消息多保存几份。
- 设置 min.insync.replicas > 1。Broker 端参数，消息至少要被写入到多少个副本才算是“已提交”。
- replication.factor > min.insync.replicas。推荐 replication.factor = min.insync.replicas + 1。
- 消费者端 enable.auto.commit = false，手动提交位移。
