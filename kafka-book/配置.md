# 部署环境

使用普通磁盘组成存储空间即可。使用机械磁盘完全能够胜任 Kafka 线上环境。假设业务每天发 1 亿条消息，两个副本，保存两周，平均每条消息占用磁盘 1KB。估算一下总的空间大小是 200GB 每天。如果预留 10%的空间给消息之外的其他数据，则是 220G 每天。乘以 14，在乘以 0.75 的压缩比（假设），大概需要 2.25TB。

对于 kafka 这种通过网络进行大量数据传输的框架而言，带宽很容易成为瓶颈。带宽不足导致的性能问题占一半以上。以 1Gbps 的千兆网络为例，假设你现在的业务目标是在 1 小时内处理 1TB 的业务数据，请问你需要多少台 kafka 服务器？由于带宽是 1Gbps，即每秒处理 1G 数据，假设 kafka 能用到 70%的带宽资源，也就是 700Mbps。你还不能让 kafka 常规性地使用这么大带宽，通常只有三分之一，大约 240Mbps。这样，我们大概可以算出一小时处理 1TB 数据需要 10 台 kafka。如果你希望数据一共有三个副本，则是 30 台。

### 关键配置

- 静态参数：必须在配置文件 server.properties 中设置的参数，重启才生效。
- 主题级别参数的设置则有所不同，通过 kafka-configs 命令来修改它们。
- JVM 和操作系统级别参数的设置方法比较通用化。

### Broker 存储相关

- log.dirs：这是非常重要的参数，指定了 Broker 需要使用的若干个文件目录路径。
- log.dir：注意这是 dir，结尾没有 s，说明它只能表示单个路径，它是补充上一个参数用的。
- 只设置 log.dirs 就好，不要设置 log.dir。线上生产环境中一定要为 log.dirs 配置多个路径。

### zk 相关

Kafka 与 Zk 相关的最重要的参数当属 **zookeeper.connect**。比如 zk1:2181,zk2:2181,zk3:2181。2181 是 ZooKeeper 的默认端口。如果你有两套 Kafka 集群，假设分别叫它们 kafka1 和 kafka2，那么两套集群的 zookeeper.connect 参数可以这样指定：zk1:2181,zk2:2181,zk3:2181/kafka1 和 zk1:2181,zk2:2181,zk3:2181/kafka2。切记 chroot 只需要写一次，而且是加到最后的。

### broker 连接相关

- listeners：监听器，告诉外部连接者通过什么协议访问指定主机名和端口开放的 Kafka 服务。
- advertised.listeners：这组监听器是 Broker 用于对外发布的。
- host.name/port：过期参数了，压根不要为它们指定值。

主机名这个设置中最好全部使用主机名，即 Broker 端和 Client 端应用配置中全部填写主机名。

### topic 管理

- auto.create.topics.enable：是否允许自动创建 Topic。最好是 false。
- unclean.leader.election.enable：是否允许 Unclean Leader 选举。最好是 false。
- auto.leader.rebalance.enable：是否允许定期进行 Leader 选举。最好是 false。

### 数据留存方面

- log.retention.{hours|minutes|ms}：都是控制一条消息数据被保存多长时间。通常设置 hours，比如 log.retention.hours=168 表示保存 7 天的数据，自动删除 7 天前数据。
- log.retention.bytes：指定 Broker 为消息保存的总磁盘容量大小。默认 -1（不设限）。
- message.max.bytes：控制 Broker 能接收的最大消息大小。默认 1000012 太少了。

### topic 级别参数

- retention.ms：规定了该 Topic 消息被保存的时长。默认是 7 天，即该 Topic 只保存最近 7 天的消息。一旦设置了这个值，它会覆盖掉 Broker 端的全局参数值。
- retention.bytes：规定了要为该 Topic 预留多大的磁盘空间。和全局参数作用相似，这个值通常在多租户的 Kafka 集群中会有用武之地。当前默认值是 -1，表示可以无限使用磁盘空间。

```sh
# 创建topic
bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --topic transaction --partitions 1 --replication-factor 1 --config retention.ms=15552000000 --config max.message.bytes=5242880

# 修改topic设置
bin/kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name transaction --alter --add-config max.message.bytes=10485760
```

### JVM 参数

如果 Broker 所在机器的 CPU 资源非常充裕，建议使用 CMS 收集器。启用方法是指定-XX:+UseCurrentMarkSweepGC。否则，使用吞吐量收集器。开启方法是指定-XX:+UseParallelGC。 **如果你已经在使用 Java 8 了，那么就用默认的 G1 收集器就好了**。在没有任何调优的情况下，G1 表现得要比 CMS 出色，主要体现在更少的 Full GC，需要调整的参数更少等，所以使用 G1 就好了。

```sh
$> export KAFKA_HEAP_OPTS=--Xms6g  --Xmx6g
$> export KAFKA_JVM_PERFORMANCE_OPTS= -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true
$> bin/kafka-server-start.sh config/server.properties
```

一个无脑通用的建议：将 JVM 堆设置成 6GB，这是目前业界比较公认的一个合理值。默认的 1GB 有点太小。毕竟 Kafka Broker 在与客户端进行交互时会在 JVM 堆上创建大量的 ByteBuffer 实例，Heap Size 不能太小。

### 操作系统参数

- 首先是 ulimit -n。通常将它设置成一个超大的值是合理，比如 ulimit -n 1000000。
- 文件系统。如 ext3、ext4 或 XFS 这样的日志型文件系统。生产环境最好还是使用 XFS。
- swap 的调优。建议将 swappniess 配置成一个接近 0 但不为 0 的值，比如 1。
- 提交时间或者说是 Flush 落盘时间。向 Kafka 发送数据并不是真要等数据被写入磁盘才会认为成功，而是只要数据被写入到操作系统的页缓存（Page Cache）上就可以了，随后操作系统根据 LRU 算法会定期将页缓存上的“脏”数据落盘到物理磁盘上。这个定期就是由提交时间来确定的，默认是 5 秒。一般情况下我们会认为这个时间太频繁了，可以适当地增加提交间隔来降低物理磁盘的写操作。当然你可能会有这样的疑问：如果在页缓存中的数据在写入到磁盘前机器宕机了，那岂不是数据就丢失了。的确，这种情况数据确实就丢失了，但鉴于 Kafka 在软件层面已经提供了多副本的冗余机制，因此这里稍微拉大提交间隔去换取性能还是一个合理的做法。

# 无消息丢失配置

**Kafka 只对“已提交”的消息（committed message）做有限度的持久化保证.** 当 Kafka 的若干个 Broker 成功地接收到一条消息并写入到日志文件后，它们会告诉生产者程序这条消息已成功提交。此时，这条消息在 Kafka 看来就正式变为“已提交”消息了。那为什么是若干个 Broker 呢？这取决于你对“已提交”的定义。你可以选择只要有一个 Broker 成功保存该消息就算是已提交，也可以是令所有 Broker 都成功保存该消息才算是已提交。不论哪种情况，Kafka 只对已提交的消息做持久化保证这件事情是不变的。

### 消息丢失案例

案例 1：Producer 程序丢失消息。这应该算是被抱怨最多的数据丢失场景了。Producer 是异步发送消息的，调用 producer.send(msg) 这个 API 是 fire and forget 的，通常会立即返回，但不能认为消息发送已完成。

解决此问题的方法非常简单：**使用 producer.send(msg, callback)，不要使用 producer.send(msg)**。一旦出现消息提交失败的情况，你就可以有针对性地进行处理。

举例来说，如果是因为那些瞬时错误，那么仅仅让 Producer 重试就可以了；如果是消息不合格造成的，那么可以调整消息格式后再次发送。总之，处理发送失败的责任在 Producer 端而非 Broker 端。你可能会问，发送失败真的没可能是由 Broker 端的问题造成的吗？当然可能！如果你所有的 Broker 都宕机了，那么无论 Producer 端怎么重试都会失败的，此时你要做的是赶快处理 Broker 端的问题。但之前说的核心论据在这里依然是成立的：Kafka 依然不认为这条消息属于已提交消息，故对它不做任何持久化保证。

案例 2 ：Consumer 程序丢失数据。解决办法很简单：**先消费消息，再更新位移**。这种方式带来的问题是消息的重复处理。如果是多线程异步处理消费消息，Consumer 程序不要开启自动提交位移，而是要应用程序手动提交位移。**单个 Consumer 程序使用多线程来消费消息说起来容易，写成代码却异常困难，因为你很难正确地处理位移的更新，也就是说避免无消费消息丢失很简单，但极易出现消息被消费了多次的情况**。

### 最佳实践

- 使用 producer.send(msg, callback)。要有回调。
- 设置 acks = all。生产者参数。表明所有副本 Broker 都要接收到消息，该消息才算是“已提交”。
- 设置 retries（生产者参数）为一个较大的值。当出现网络的瞬时抖动时，自动重试发送消息。
- 设置 unclean.leader.election.enable = false。（Broker 参数，不让不干净的节点参加竞选）
- 设置 replication.factor >= 3。Broker 端的参数。消息多保存几份。
- 设置 min.insync.replicas > 1。Broker 端参数，消息至少要被写入到多少个副本才算是“已提交”。
- replication.factor > min.insync.replicas。推荐 replication.factor = min.insync.replicas + 1。
- 消费者端 enable.auto.commit = false，手动提交位移。
