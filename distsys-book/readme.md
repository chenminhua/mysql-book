之所以需要分布式系统，关键在于提升两个方面的指标：Scalability 与 Avaliability，其中可扩展性需求保证了我们能服务更多的用户，在单位时间内完成更大的业务量。而可用性则是保证了我们的系统能无间断的向用户提供服务。

Scalability 方面，一种是无状态业务进程的 Scalability，这种相对比较容易，在需要扩容的时候增加实例就可以了。另一种比较麻烦的是有状态服务的扩容，业界常用的方案有数据主从复制的方案，也有将数据进行 partition 并分布到不同的机器上的方案等等。

Availability 方面，关键在于消除单点。值得注意的是，单点可能在任意维度出现，单服务实例是单点，单数据库是单点，单机房也是单点，要提升 Availability 就需要在各个维度上消除可能存在的单点。

关于微服务架构，我推荐一篇 Uncle Bob 的文章《Clean Micro-service Architecture》https://blog.cleancoder.com/uncle-bob/2014/10/01/CleanMicroserviceArchitecture.html 。文章中有一部分内容给我启发很多，Uncle Bob 认为，微服务架构不过是一种 deployment option，而好的架构在代码层面不应该关心软件是被如何部署的。

换言之，各个服务是以一台服务器上的一组进程的方式启动，还是运行在 kubernetes 集群上，不应该影响代码的编写。被编写好的一组进程，应该可以以各种不同的方式被组合在一起，或者说编排在一起。我们最近就遇到一些将若干服务组合起来部署到其他环境中的需求，而且对于运行时的需求其实各个环境是不一样的。

分布式系统也会带来不少麻烦，比如运维的复杂度变高，测试和 debug 的困难增加，需要监控的目标和指标数增加。同时对于顶层设计能力的要求也变高了，你需要在一些关键指标之间做 trade-off，寻找对业务最关键的点。尽可能保证各个业务能够 fail independently，同时对于不能 fail 的服务则要做到高可用。

分布式系统异构导致的不标准问题，这个问题还是个纪律问题，好的软件项目需要大量的纪律，表的设计，接口的设计，日志的格式，协议的使用，命名空间的划分，配置的管理，这些都是纪律问题。

服务依赖问题就是大家很爱说的解耦。耦合是一个很常见的事，基本上来说，如果你知道一个服务是怎么做的，你就依赖了他，只是耦合轻或重的问题。

如果你直接调用了它的接口，你就很重的依赖了它，因为，如果它挂了你就走不下去了（你的可用性肯定比它低），如果它响应慢你就也跟着慢。如果你和它之间是消息队列，这时候如果你不知道它存在，你就不依赖它了，但它是依赖你的，你哪天调用消息队列的代码出了 bug，它的业务就受影响了。

解决服务依赖问题的关键我认为在于，尽可能让每个服务能够 fail independently, 基于对方可能无法提供服务的假设来编程。而对于最基础的不能挂的服务，一定要做高可用。

关于故障，分布式服务出故障的概率是很高的。可怕的不是出故障，而是故障恢复时间过长，以及故障影响面过大。某个报表导出服务挂一会儿问题可能不大，但是用户中心如果挂了，可能整个系统都会收影响。对于那些特别重要的服务，防灾比救灾更重要，我们需要引入更多的保护机制，比如限流，降级等策略，防止核心服务被打挂。

通常我们可以把整个系统分为四层：基础层，中间层，应用层，接入层。每一层出现问题都可能导致整个系统不可用，在层与层间需要设立规范。

- 基础层包括机器，存储等基础设施；
- 中间层则是 kafka，redis 等中间件；
- 应用层是我们的应用服务；
- 接入层是网关，LB, CDN，DNS 等。

## 分布式系统的关键技术

分布式系统解决的第一个问题是大流量处理，通常可以分为并发读和并发写的问题。

1.首先我们可以想到的思路是 scale out，即扩大集群规模。对于无状态的业务服务，通过增加服务器并利用负载均衡就能实现。对于有状态的服务，比如数据库、消息队列这种，就要通过 partition 机制，或者主从复制的机制来 scale out。

2.除了 scale out 外，对于读多的业务，我们还可以引入缓存层，减少对后端服务与存储的访问。

3.对于一些对一致性要求不高的业务，我们可以采用异步的方式，可以做到削峰去谷，在大流量出现时，可以避免上下游间相互影响。

分布式系统解决的另一个问题则是稳定性问题，也就是提升可用性，提高 SLA 的问题。这个问题也应该从多个维度来考虑。

首先我们要对业务进行拆分，不同的业务对于可用性的要求也是不一样的，对可用性要求高的业务不可以依赖对可用性要求低的业务。此外，我们应该对服务做冗余，并实现弹性伸缩。

对于关键业务，我们一定要进行保护，在关键时刻进行限流或降级操作，先保证基本的可用性。

此外，我们还需要高可用的运维架构。

分布式系统的关键总体分为：
全栈系统监控，服务/资源调度，流量调度，状态/数据调度，开发运维的自动化。
比如全栈系统监控这块，包括了基础设施的监控（磁盘 IO,网络 IO，CPU load 等等），中间件的监控，应用层的监控。监控应该有一个监控中心，用于看到整个集群的健康状态。
服务需要具备，4 个接口，start，stop，appconfig，healthcheck。

### 分布式系统的关键技术

而通过上面的分析，我们可以看到，引入分布式系统，会引入一堆技术问题，需要从以下几个方面来解决。

- 服务治理。服务拆分、服务调用、服务发现、服务依赖、服务的关键度定义……服务治理的最大意义是需要把服务间的依赖关系、服务调用链，以及关键的服务给梳理出来，并对这些服务进行性能和可用性方面的管理。
- 架构软件管理。服务之间有依赖，而且有兼容性问题，所以，整体服务所形成的架构需要有架构版本管理、整体架构的生命周期管理，以及对服务的编排、聚合、事务处理等服务调度功能。
- DevOps。分布式系统可以更为快速地更新服务，但是对于服务的测试和部署都会是挑战。所以，还需要 DevOps 的全流程，其中包括环境构建、持续集成、持续部署等。
- 自动化运维。有了 DevOps 后，我们就可以对服务进行自动伸缩、故障迁移、配置管理、状态管理等一系列的自动化运维技术了。
- 资源调度管理。应用层的自动化运维需要基础层的调度支持，也就是云计算 IaaS 层的计算、存储、网络等资源调度、隔离和管理。
- 整体架构监控。如果没有一个好的监控系统，那么自动化运维和资源调度管理只可能成为一个泡影，因为监控系统是你的眼睛。没有眼睛，没有数据，就无法进行高效的运维。所以说，监控是非常重要的部分。这里的监控需要对三层系统（应用层、中间件层、基础层）进行监控。
- 流量控制。最后是我们的流量控制，负载均衡、服务路由、熔断、降级、限流等和流量相关的调度都会在这里，包括灰度发布之类的功能也在这里。

# 分布式系统与 paxos

分布式系统的特点： 分布性，对等性，并发性，缺乏全局时钟，故障总会发生

分布式系统环境问题： 通信异常，节点故障，网络分区，三态问题。

BASE: basically available, soft state and eventually consistent。 基本可用，软状态，最终一致。

- 基本可用：响应时间上的损失，功能上的损失。
- 弱状态：允许系统在不同节点的副本间同步数据过程中存在延时。
- 最终一致：系统中所有副本在经过一段时间同步后，最终能够达到一致的状态。

## 一致性协议

2PC, 先准备后提交。

2PC 的缺点：1. 同步阻塞，2. 存在单点，如果有人没收到提交请求，则还是会有数据不一致问题。

3PC, CanCommit -> PreCommit -> Commit

Paxos, 1990 年由 leslie lamport（2013 年图灵奖得主） 提出。拜占庭将军问题。《Paxos made simple》。
在古希腊有个叫 paxos 的岛，岛上采用议会形式来通过法令，议员们通过信使来传递消息。议员和信使都可能随时离开，并且信使可能传递重复的消息或丢弃消息。

Raft

## chubby: paxos 的工程实践

chubby 是 google 推出的分布式锁服务，GFS 和 Big Table 等大型系统都用它来解决分布式协作，元数据存储和 master 选举等问题。

《the chubby lock service for loosly-coupled distributed systems》

通过投票选举产生一个获得过半投票的服务器为 master，一旦产生了 master，chubby 就会保证在这段时间内不会再有其他服务器成为 master，这段时间称为租期（master lease）。

在运行过程中，master 会不停通过续租的方式来延长租期，如果 master 服务器出现故障，就会发生新一轮的 Master 选举。

集群中每个服务器都维护一个副本，但是只有 master 有权利 update，其他服务器只能从 master 同步更新。

chubby 客户端只和 master 进行通信，所有请求都发到 master,针对写，master 会广播给其他服务器，而读则由 master 亲自搞定。

## Quorum 机制： 过半机制
