当一个网络包来了的时候，网卡会通过 DMA 方式将网络包收入队列，然后通过硬中断告诉中断程序，网卡中断处理程序会将网络帧拷贝到 sk_buff 缓冲区，然后通过软中断通知内核。内核协议栈从缓冲区中取出网络帧，并通过网络协议栈处理这个网络帧。最后将数据拷贝到 socket 的接收缓存中。

发送一个包的时候，应用程序先调用 socket api，通过系统调用陷入内核，然后将数据包放到 socket 缓冲区中。然后，内核的网络协议栈从 socket 发送缓冲区取出数据包，进行层层封装，比如增加 TCP/IP 头，查找下一跳的 IP，并按照 MTU 大小进行分片。分片后的网络包送到网络接口层，进行物理寻址，找到下一跳的 MAC 地址，然后再封一层，并放入发包队列。之后通过软中断通知网卡的驱动程序，并由驱动程序通过 DMA 将包读出并通过物理网卡发送出去。

## 网络性能指标

带宽，吞吐量，延时，PPS，并发连接数，丢包率，重传率。

## C10K, C1000K, C10M

如何用一个线程处理多个请求，也就是一个线程响应多个 IO？

IO 事件通知的方式：水平触发和边缘触发。

- 水平触发： 应用程序随时检查文件描述符的状态，并根据状态来决定是否进行 IO 操作。
- 边缘触发： 只有在文件描述符状态改变时，才发送通知。

select 或者 poll 都是采用非阻塞 IO + 水平触发通知。即从文件描述符列表中找到有 IO 的进行 IO 操作。由于 IO 是非阻塞的，一个线程就可以同时监控一批套接字的 FD。

但是 select 和 poll 的缺点在于轮询比较耗时。select 还有文件描述符数量的限制，poll 虽然没有文件描述符限制，但是依然需要轮询。此外，select 和 poll 还需要把 FD SET 传入内核空间，当发生 IO 事件后还要传回内核空间，这增加了处理成本。

epoll 使用非阻塞 IO + 边缘触发。epoll 使用红黑树在内核中管理文件描述符的集合。并且使用事件驱动机制，只关注 IO 事件的文件描述符，而不用轮询。

除了上面两种方式外，还有一种异步 IO，简称 AIO。异步 IO 允许应用程序同时发起很多 IO 操作而不用等待这些操作完成，而在 IO 完成后系统通过事件通知方式告诉应用程序。

ps: 关于异步 IO，存在很多迷思。举个例子来说，大家都说 NodeJS 是异步 IO 的。其实这个说法并不完全准确，NodeJS 在 Linux 上的网络 IO 依然是采用的 epoll 这种 「IO 多路复用」的机制，而非异步 IO。但是对于 NodeJS 的用户线程（或者说调用者写的程序）来说，IO 确实是异步的。我认为比较准确的表述是，NodeJS 的用户程序与 NodeJS 的 Runtime 之间的 IO 调用是异步 IO，而 NodeJS 的 Runtime 与操作系统之间则不是。

到 C1000K 的时候，问题就变得复杂了。

- 假如每个请求需要 16KB 内存，那总共就要 15GB 内存。
- 吞吐的要求也很高，至少需要配置万兆网卡。
- 文件描述符数量、连接状态跟踪，网络协议栈的缓存大小等等。
- 大量请求带来的中断处理也是不小的成本。
