深度强化学习
深度强化学习将深度学习的感知能力和强化学习的决策能力结合。可以分成三类，分别是基于价值、基于策略和基于模型的深度强化学习。
基于价值(value-based)的深度强化学习的基本思路是建立⼀个价值函数的表示。
价值函数 (value function)通常被称为 Q 函数，Q(s,a)。但对价值函数的最优化的真正⽬的是确定行动策略。
策略是从状态空间到动作空间的映射，表示智能体在状态 st 下选择动作 a，执行这一动作并以概率 Pa(st, st+1)转移到下一状态 st+1，同时接受来自环境的奖赏 Ra(st, st+1)。
价值函数和策略的关系在于它可以表示智能体一直执行某个固定策略所能获得的累计回报。如果某个策略在所有 state-action 组合上的期望回报优于所有其他策略，这个策略就是最佳策略。而基于价值的深度强化学习就是要通过价值函数来找到最优策略。
没有”深度“的强化学习中，使用价值函数的算法叫做 Q-learning，其就是在每个状态下执行不同动作，来观察得到的奖励，并迭代执行这个操作。
本质上说，Q-learning 是有限集上的搜索方法，如果出现一个不在原始集合中的状态，Q 算法就无能为力了。Q-learning 的泛化能力较差，不能对未知情况进行预测。
为了实现可预测的 Q-learning，深度强化学习采用的方式是将 Q-learning 算法的参数也作为未知变量，用神经网络训练 Q 算法的参数。
深度 Q 网络中有两种机制：经验回放和目标 Q 网络。
经验回放通过将以往的状态转移数据存储下来作为训练数据使用，以此来克服数据之间的相关性，避免网络收敛到局部最小值。
目标 Q 网络则对当前 Q 值和目标 Q 值做了区分，单独使用一个新网络来产生目标 Q 值。这相当于对当前 Q 和目标 Q 进行去相关，从而克服了非平稳目标函数的影响，避免算法得到震荡的结果。
